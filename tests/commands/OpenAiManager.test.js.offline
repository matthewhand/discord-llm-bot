const axios = require('axios');
jest.mock('axios');

// Mocking configurationManager to return specific configurations for testing
jest.mock('../../src/config/configurationManager', () => ({
  getConfig: jest.fn((key) => {
    const mockConfigs = {
      'BOT_USER_ID': 'mock-bot-id',
      'LLM_TEMPERATURE': 0.5,
      'LLM_MAX_TOKENS': 100,
      'LLM_TOP_P': 1.0,
      'LLM_FREQUENCY_PENALTY': 0.0,
      'LLM_PRESENCE_PENALTY': 0.0,
      'LLM_ENDPOINT_URL': "http://mockendpoint.com/v1/chat/completions",
      'LLM_API_KEY': "mock-api-key",
    };
    return mockConfigs[key];
  }),
}));

// Assuming constants are used directly in your tests or manager
const constants = require('../../src/config/constants');

let OpenAiManager;

beforeEach(() => {
  jest.resetModules(); // Clears the Jest cache
  axios.post.mockClear();
  // Dynamically require the OpenAiManager to ensure mocks are applied
  OpenAiManager = require('../../src/managers/OpenAiManager').default;
});

describe('OAI API Manager Tests', () => {
  let oaiApiManager;

  beforeEach(() => {
    // Instantiate the OpenAiManager for each test to ensure a clean state
    oaiApiManager = new OpenAiManager();
  });

  it('sends a request and receives a successful response', async () => {
    const mockResponse = { data: { choices: [{ message: { content: 'Test response' } }] } };
    axios.post.mockResolvedValue(mockResponse);

    const requestBody = {
      model: 'text-davinci-003',
      messages: ["Hello, world!"],
      temperature: constants.LLM_TEMPERATURE,
      max_tokens: constants.LLM_MAX_TOKENS,
      top_p: constants.LLM_TOP_P,
      frequency_penalty: constants.LLM_FREQUENCY_PENALTY,
      presence_penalty: constants.LLM_PRESENCE_PENALTY,
    };

    const response = await oaiApiManager.sendRequest(requestBody);

    expect(response).toEqual(mockResponse.data);
    expect(axios.post).toHaveBeenCalledWith(
      constants.LLM_ENDPOINT_URL,
      expect.objectContaining(requestBody),
      {
        headers: {
          'Authorization': `Bearer ${constants.LLM_API_KEY}`,
          'Content-Type': 'application/json',
        }
      }
    );
  });

  it('correctly formats the request body with chat history', () => {
    const historyMessages = [
      { userId: 'user1', content: "How are you?" },
      { userId: 'mock-bot-id', content: "I'm fine, thanks!" }
    ];
    const userMessage = "What's the weather like today?";
    const model = 'text-davinci-003';

    const requestBody = oaiApiManager.buildRequestBody(historyMessages, userMessage, model);

    expect(requestBody).toMatchObject({
      model,
      messages: expect.arrayContaining([
        { role: 'system', content: constants.SYSTEM_PROMPT },
        { role: 'user', content: "How are you?" },
        { role: 'assistant', content: "I'm fine, thanks!" },
        { role: 'user', content: userMessage }
      ]),
      temperature: constants.LLM_TEMPERATURE,
      max_tokens: constants.LLM_MAX_TOKENS,
      top_p: constants.LLM_TOP_P,
      frequency_penalty: constants.LLM_FREQUENCY_PENALTY,
      presence_penalty: constants.LLM_PRESENCE_PENALTY,
    });
  });
});
