const constants = {
  LLM_API_KEY: 'fake-api-key',
  LLM_MODEL: "text-davinci-003",
  LLM_SYSTEM_PROMPT: "You are a helpful assistant.",
  LLM_RESPONSE_MAX_TOKENS: 150,
  LLM_ENDPOINT_URL: "https://api.openai.com"
};

// Mocking the OpenAI library to simulate a constructor behavior correctly.
// The OpenAI object needs to mimic an actual class with a completions.create method.
jest.mock('openai', () => {
  const mockCompletionsCreate = jest.fn().mockResolvedValue({
    choices: [{ text: 'Test response', finish_reason: 'length' }]
  });

  // OpenAI is mocked as a constructor function that returns an object with a completions property.
  return {
    OpenAI: jest.fn().mockImplementation(() => {
      return { completions: { create: mockCompletionsCreate }};
    })
  };
});

const { OpenAI } = require("openai");
const OpenAiManager = require('../../src/managers/OpenAiManager');
const LLMResponse = require('../../src/interfaces/LLMResponse');

describe('OpenAiManager', () => {
  let openAiManager;

  beforeEach(() => {
    OpenAI.mockClear();
    openAiManager = new OpenAiManager();
  });

  // Test to ensure that the OpenAI client is instantiated with correct API parameters.
  it('instantiates OpenAI with the correct parameters', () => {
    expect(OpenAI).toHaveBeenCalledTimes(1);  // Asserts that the constructor was called exactly once.
    expect(OpenAI).toHaveBeenCalledWith({
      apiKey: constants.LLM_API_KEY,
      baseURL: constants.LLM_ENDPOINT_URL
    });
  });

  // Test to verify that the payload is structured correctly following specific message role sequences.
  it('structures payload correctly for alternating roles after system messages', () => {
    const historyMessages = [
      { getText: () => "System prompt", isFromBot: () => true },
      { getText: () => "User message 1", isFromBot: () => false },
      { getText: () => "Assistant response", isFromBot: () => true },
      { getText: () => "User message 2", isFromBot: () => false }
    ];

    const expectedPayload = {
      model: constants.LLM_MODEL,
      max_tokens: constants.LLM_RESPONSE_MAX_TOKENS,
      messages: [
        { role: 'system', content: "System prompt" },
        { role: 'user', content: "User message 1" },
        { role: 'assistant', content: "Assistant response" },
        { role: 'user', content: "User message 2" },
      ]
    };

    // This call assumes the implementation of buildRequestBody to prepare data correctly based on roles.
    const result = openAiManager.buildRequestBody(historyMessages);
    expect(result).toEqual(expectedPayload);
  });

  // Test to check if the sendRequest method processes the API call correctly and returns a structured LLMResponse.
  it('returns an LLMResponse object with the correct properties after a successful API call', async () => {
    const requestBody = {
      model: constants.LLM_MODEL,
      prompt: "Tell me a joke",
      max_tokens: constants.LLM_RESPONSE_MAX_TOKENS
    };
    const response = await openAiManager.sendRequest(requestBody);
    expect(response).toBeInstanceOf(LLMResponse);  // The response should be an instance of LLMResponse.
    expect(response.content).toEqual('Test response');  // Checking the response content.
    expect(response.finishReason).toEqual('length');  // Verifying the reason for the response completion.
    expect(response.tokensUsed).toBeLessThan(requestBody.max_tokens);  // Ensuring token usage is within limits.
  });
});
