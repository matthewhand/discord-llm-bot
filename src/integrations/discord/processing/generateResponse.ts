import Debug from 'debug';
import { OpenAiService } from '@src/integrations/openai/OpenAiService';
import { sendRequest } from '@src/integrations/openai/operations/sendRequest';
import LLMResponse from '@src/llm/interfaces/LLMResponse';
import ConfigurationManager from '@config/ConfigurationManager';

const debug = Debug('app:createAiResponse');

/**
 * Creates a response using the OpenAiService.
 *
 * This function submits a transcript as input to the OpenAiService and retrieves a generated response based on the provided content.
 *
 * Key Features:
 * - Utilizes the sendRequest function to communicate with the OpenAiService API.
 * - Handles and logs errors during the API request, ensuring robust error management.
 * - Returns the generated response text or undefined in case of an error.
 *
 * @param {OpenAiService} aiService - The OpenAiService instance responsible for handling the API request.
 * @param {string} transcript - The input text to be processed and responded to by the OpenAiService.
 * @returns {Promise<string | undefined>} The response generated by the OpenAiService, or undefined if an error occurs.
 */
export async function createAiResponse(aiService: OpenAiService, transcript: string): Promise<string | undefined> {
    try {
        const response: LLMResponse = await sendRequest(aiService, {
            model: aiService.getModel(),
            messages: [{ role: 'user', content: transcript }],
            max_tokens: ConfigurationManager.OPENAI_MAX_TOKENS,
            temperature: ConfigurationManager.OPENAI_TEMPERATURE,
            top_p: ConfigurationManager.LLM_TOP_P,
            frequency_penalty: ConfigurationManager.OPENAI_FREQUENCY_PENALTY,
            presence_penalty: ConfigurationManager.OPENAI_PRESENCE_PENALTY,
        });
        return response.getContent();
    } catch (error: any) {
        debug('Error generating response: ' + (error instanceof Error ? error.message : String(error)));
        return undefined;
    }
}
